{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8263beae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\uddin.b\\\\PycharmProjects\\\\software\\\\dataset'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ad6711d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "from itertools import zip_longest\n",
    "import time\n",
    "from pathlib import Path\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62c2f4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\uddin.b\\PycharmProjects\\software\n"
     ]
    }
   ],
   "source": [
    "print(\"Current working directory: {0}\".format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cd8e4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\uddin.b\\PycharmProjects\\software\\dataset\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"dataset/\")\n",
    "print(\"Current working directory: {0}\".format(os.getcwd()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27321b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(input_file, output_file):\n",
    "    '''\n",
    "\n",
    "    :param input_file: text file containing the analysis results of ansys\n",
    "    :param output_file: empty text file\n",
    "    :return: text file (output_file being filled) after removal of extra words and spaces\n",
    "    '''\n",
    "    file1 = open(input_file, 'r')\n",
    "    file2 = open(output_file, 'w')\n",
    "\n",
    "    for line in file1:\n",
    "\n",
    "        # reading all lines that begin\n",
    "\n",
    "        x = re.match(\"^([A-Za-z]|\\/|\\*|\\=)\", re.sub(r\"^[ \\t\\r\\n]+\", \"\", line))\n",
    "        y = re.search('\\S', line)\n",
    "        if not x and y:\n",
    "            file2.write(line)\n",
    "\n",
    "    file1.close()\n",
    "    file2.close()\n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d9ada17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rivet_to_df(input_file):\n",
    "    '''\n",
    "    \n",
    "    :param input_file: cleaned output file from function clean_text without any unnecessary text\n",
    "    :return: dataframe with 100000 rows as in total events and n_11 and n_13 as columns to show number of these particles in every event\n",
    "             other columns are 'ID_211', 'n_211', 'meanPt_211','meaneta_211', 'varPt_211', 'vareta_211',\n",
    "               'ID_22', 'n_22', 'meanPt_22', 'meaneta_22','varPt_22', 'vareta_22'\n",
    "               \n",
    "    \n",
    "    '''\n",
    "    ##part1 : creates a dataframe df1 with n_11 and n_13 as columns to show number of these particles in every event\n",
    "    #contains the file as list of list of floats\n",
    "    data = []\n",
    "\n",
    "    #leading and trailing spaces and commas are removed,also string is converted to floats\n",
    "    with open(input_file) as file:\n",
    "        for line in file:\n",
    "            event = []\n",
    "            line = line[:-1].replace(',\\n', '\\n')\n",
    "            line = line.rstrip()\n",
    "            line = line.rstrip(',')\n",
    "            #print(line)\n",
    "\n",
    "            for numstr in line.split(\",\"):\n",
    "                if numstr:\n",
    "                    try:\n",
    "                        numFl = float(numstr)\n",
    "                        event.append(numFl)\n",
    "\n",
    "\n",
    "                    except ValueError as e:\n",
    "                        print(e)\n",
    "            data.append(event)\n",
    "    file.close()\n",
    "    #list of dictionieries of size 100000.Each dictionery store n_11 and n_13 for each event\n",
    "    l =[]\n",
    "    for item in data:\n",
    "        d = {'n_11':0,'n_13':0}\n",
    "        for x in item[::6]:\n",
    "\n",
    "            if int(x) == 11:\n",
    "                d['n_11'] =item[item.index(x) + 1]\n",
    "            if int(x) == 13:\n",
    "                d['n_13'] =item[item.index(x) + 1]\n",
    "\n",
    "        l.append(d)\n",
    "    #list of dictioneries is converted to df\n",
    "    df1 = pd.DataFrame(l,columns = ['n_11','n_13'])\n",
    "    \n",
    "    ##part 2: find out common particles in each event i.e. 22 and 211 and create a dataframe df2 with columns\n",
    "     ##'ID_211', 'n_211', 'meanPt_211','meaneta_211', 'varPt_211', 'vareta_211',\n",
    "       ##        'ID_22', 'n_22', 'meanPt_22', 'meaneta_22','varPt_22', 'vareta_22'\n",
    "    \n",
    "    chunks = {}\n",
    "    first_elements = []\n",
    "    count = 0\n",
    "    event_ids = []\n",
    "\n",
    "    for item in data:\n",
    "\n",
    "        #item.pop()\n",
    "        chunks[count] = [list(np.float_(item[x:x+6])) for x in range(0, len(item), 6)]\n",
    "        event_ids.append(chunks[count][0])\n",
    "        count += 1\n",
    "    #list of dictionaries with particle ids as keys\n",
    "    d = []\n",
    "    i = range(len(chunks))\n",
    "    for item in i:\n",
    "        l = {}\n",
    "        for lists in chunks[item]:\n",
    "            l[lists[0]] = lists[1:]\n",
    "        d.append(l)\n",
    "    #particles id in each event\n",
    "    particles_event = []\n",
    "    for item in d:\n",
    "        particles_event.append(list(item.keys()))\n",
    "    # common particles in each event\n",
    "    elements_in_all = list(set.intersection(*map(set, particles_event)))\n",
    "    #list of dictionaries with two keys i.e. 211 and 22 for each event\n",
    "    d2 = []\n",
    "    for item in d:\n",
    "\n",
    "        item = dict((k, item[k]) for k in elements_in_all if k in item)\n",
    "        d2.append(item)\n",
    "    #list of list with each sublist is the [211,total number ,px,py,pz,22.otal number ,px,py,pz]\n",
    "    dictlist = []\n",
    "    for item in d2:\n",
    "        temp = []\n",
    "\n",
    "        for key, value in item.items():\n",
    "            temp = value\n",
    "            temp.insert(0, key)\n",
    "            dictlist.append(temp)\n",
    "    x = iter(dictlist)\n",
    "    dictlist= [a+b for a, b in zip_longest(x, x, fillvalue=[])]\n",
    "    columns = ['ID_211', 'n_211', 'meanPt_211','meaneta_211', 'varPt_211', 'vareta_211',\n",
    "               'ID_22', 'n_22', 'meanPt_22', 'meaneta_22','varPt_22', 'vareta_22']\n",
    "    df2 =  pd.DataFrame(dictlist, columns=columns)\n",
    "    df2.drop(['ID_211', 'ID_22'], axis=1, inplace=True)\n",
    "    df2.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    ##merging df1 and df2 together to get df\n",
    "    datasets = [df1, df2]\n",
    "    df = pd.concat(datasets,axis =1)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1214e8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = clean_text('background7var.txt','a.txt')\n",
    "b = clean_text('signal7var.txt','b.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42b1dd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa = rivet_to_df(a)\n",
    "dfb = rivet_to_df(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a24b361f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_11</th>\n",
       "      <th>n_13</th>\n",
       "      <th>n_211</th>\n",
       "      <th>meanPt_211</th>\n",
       "      <th>meaneta_211</th>\n",
       "      <th>varPt_211</th>\n",
       "      <th>vareta_211</th>\n",
       "      <th>n_22</th>\n",
       "      <th>meanPt_22</th>\n",
       "      <th>meaneta_22</th>\n",
       "      <th>varPt_22</th>\n",
       "      <th>vareta_22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>1.200300</td>\n",
       "      <td>2.22773</td>\n",
       "      <td>4.15486</td>\n",
       "      <td>2.88133</td>\n",
       "      <td>253.0</td>\n",
       "      <td>0.543051</td>\n",
       "      <td>2.36261</td>\n",
       "      <td>2.186990</td>\n",
       "      <td>3.84918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1.793920</td>\n",
       "      <td>3.40869</td>\n",
       "      <td>9.46202</td>\n",
       "      <td>6.83517</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.661701</td>\n",
       "      <td>3.15288</td>\n",
       "      <td>1.311840</td>\n",
       "      <td>5.63785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>0.796928</td>\n",
       "      <td>2.54936</td>\n",
       "      <td>1.47104</td>\n",
       "      <td>3.49499</td>\n",
       "      <td>242.0</td>\n",
       "      <td>0.415353</td>\n",
       "      <td>2.37220</td>\n",
       "      <td>0.760971</td>\n",
       "      <td>4.09749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>1.082840</td>\n",
       "      <td>3.33898</td>\n",
       "      <td>3.66926</td>\n",
       "      <td>4.96105</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0.556650</td>\n",
       "      <td>2.76011</td>\n",
       "      <td>1.826720</td>\n",
       "      <td>3.25890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>1.214710</td>\n",
       "      <td>2.51976</td>\n",
       "      <td>4.50911</td>\n",
       "      <td>3.76778</td>\n",
       "      <td>199.0</td>\n",
       "      <td>0.591424</td>\n",
       "      <td>2.15625</td>\n",
       "      <td>0.987732</td>\n",
       "      <td>3.30468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>0.909074</td>\n",
       "      <td>3.00045</td>\n",
       "      <td>1.64207</td>\n",
       "      <td>3.58541</td>\n",
       "      <td>198.0</td>\n",
       "      <td>0.475206</td>\n",
       "      <td>2.88071</td>\n",
       "      <td>0.642229</td>\n",
       "      <td>3.07660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>0.869529</td>\n",
       "      <td>2.42326</td>\n",
       "      <td>1.64762</td>\n",
       "      <td>2.59428</td>\n",
       "      <td>244.0</td>\n",
       "      <td>0.429981</td>\n",
       "      <td>2.46716</td>\n",
       "      <td>0.677804</td>\n",
       "      <td>3.44386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>1.216960</td>\n",
       "      <td>2.45483</td>\n",
       "      <td>6.88666</td>\n",
       "      <td>3.49790</td>\n",
       "      <td>284.0</td>\n",
       "      <td>0.393849</td>\n",
       "      <td>2.78073</td>\n",
       "      <td>0.427200</td>\n",
       "      <td>3.72708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>1.195410</td>\n",
       "      <td>2.21723</td>\n",
       "      <td>6.37086</td>\n",
       "      <td>3.43074</td>\n",
       "      <td>197.0</td>\n",
       "      <td>0.465248</td>\n",
       "      <td>2.44946</td>\n",
       "      <td>0.597641</td>\n",
       "      <td>4.42871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>0.923375</td>\n",
       "      <td>2.74157</td>\n",
       "      <td>1.82661</td>\n",
       "      <td>3.77316</td>\n",
       "      <td>201.0</td>\n",
       "      <td>0.448209</td>\n",
       "      <td>3.00629</td>\n",
       "      <td>0.535048</td>\n",
       "      <td>4.00627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       n_11  n_13  n_211  meanPt_211  meaneta_211  varPt_211  vareta_211  \\\n",
       "0       5.0   1.0  177.0    1.200300      2.22773    4.15486     2.88133   \n",
       "1       8.0   0.0   48.0    1.793920      3.40869    9.46202     6.83517   \n",
       "2      10.0   2.0  216.0    0.796928      2.54936    1.47104     3.49499   \n",
       "3       1.0   0.0  131.0    1.082840      3.33898    3.66926     4.96105   \n",
       "4       2.0   0.0  157.0    1.214710      2.51976    4.50911     3.76778   \n",
       "...     ...   ...    ...         ...          ...        ...         ...   \n",
       "99995   2.0   1.0  188.0    0.909074      3.00045    1.64207     3.58541   \n",
       "99996   4.0   0.0  190.0    0.869529      2.42326    1.64762     2.59428   \n",
       "99997   5.0   0.0  206.0    1.216960      2.45483    6.88666     3.49790   \n",
       "99998   5.0   0.0  177.0    1.195410      2.21723    6.37086     3.43074   \n",
       "99999   3.0   1.0  167.0    0.923375      2.74157    1.82661     3.77316   \n",
       "\n",
       "        n_22  meanPt_22  meaneta_22  varPt_22  vareta_22  \n",
       "0      253.0   0.543051     2.36261  2.186990    3.84918  \n",
       "1       79.0   0.661701     3.15288  1.311840    5.63785  \n",
       "2      242.0   0.415353     2.37220  0.760971    4.09749  \n",
       "3      189.0   0.556650     2.76011  1.826720    3.25890  \n",
       "4      199.0   0.591424     2.15625  0.987732    3.30468  \n",
       "...      ...        ...         ...       ...        ...  \n",
       "99995  198.0   0.475206     2.88071  0.642229    3.07660  \n",
       "99996  244.0   0.429981     2.46716  0.677804    3.44386  \n",
       "99997  284.0   0.393849     2.78073  0.427200    3.72708  \n",
       "99998  197.0   0.465248     2.44946  0.597641    4.42871  \n",
       "99999  201.0   0.448209     3.00629  0.535048    4.00627  \n",
       "\n",
       "[100000 rows x 12 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7858a0bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_11</th>\n",
       "      <th>n_13</th>\n",
       "      <th>n_211</th>\n",
       "      <th>meanPt_211</th>\n",
       "      <th>meaneta_211</th>\n",
       "      <th>varPt_211</th>\n",
       "      <th>vareta_211</th>\n",
       "      <th>n_22</th>\n",
       "      <th>meanPt_22</th>\n",
       "      <th>meaneta_22</th>\n",
       "      <th>varPt_22</th>\n",
       "      <th>vareta_22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>1.25314</td>\n",
       "      <td>2.49607</td>\n",
       "      <td>3.01312</td>\n",
       "      <td>3.77564</td>\n",
       "      <td>210.0</td>\n",
       "      <td>0.639269</td>\n",
       "      <td>2.20267</td>\n",
       "      <td>1.99082</td>\n",
       "      <td>3.75236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_11  n_13  n_211  meanPt_211  meaneta_211  varPt_211  vareta_211   n_22  \\\n",
       "0   3.0   1.0  165.0     1.25314      2.49607    3.01312     3.77564  210.0   \n",
       "\n",
       "   meanPt_22  meaneta_22  varPt_22  vareta_22  \n",
       "0   0.639269     2.20267   1.99082    3.75236  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfb.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4cd42c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_parquet(signal_df , background_df , name ):\n",
    "    '''\n",
    "      \n",
    "\n",
    "    :param list1: df of signal i.e. status = 1 assigned here before merging with background df\n",
    "    :param list2: df of background i.e. status = 0 assigned here before merging with signal df\n",
    "    :param name : name of parquetfile to be saved\n",
    "    :return: parquet file containing background and signal data with common IDs(211,222),n_11,n_13,'meanPt_211', 'meaneta_211',\n",
    "    'varPt_211','vareta_211', 'n_22', 'meanPt_22', 'meaneta_22', 'varPt_22','vareta_22' consolidated per event\n",
    "    '''\n",
    "    columns = ['n_11', 'n_13', 'n_211', 'meanPt_211', 'meaneta_211', 'varPt_211',\n",
    "       'vareta_211', 'n_22', 'meanPt_22', 'meaneta_22', 'varPt_22',\n",
    "       'vareta_22']\n",
    "    #signal df\n",
    "    signal_df['Status'] = 1\n",
    "    #background df\n",
    "    background_df['Status'] = 0\n",
    "    datasets = [signal_df, background_df]\n",
    "    df = pd.concat(datasets)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "\n",
    "    df.to_parquet(name + '.parquet', engine='pyarrow')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4336e85",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowNotImplementedError",
     "evalue": "Support for codec 'snappy' not built",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowNotImplementedError\u001b[0m                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfb\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdfa\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlpc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36mto_parquet\u001b[1;34m(signal_df, background_df, name)\u001b[0m\n\u001b[0;32m     20\u001b[0m df\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     21\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39msample(frac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 25\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpyarrow\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\Neuro\\lib\\site-packages\\pandas\\util\\_decorators.py:207\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\Neuro\\lib\\site-packages\\pandas\\core\\frame.py:2835\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[1;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m   2749\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2750\u001b[0m \u001b[38;5;124;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[0;32m   2751\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2831\u001b[0m \u001b[38;5;124;03m>>> content = f.read()\u001b[39;00m\n\u001b[0;32m   2832\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2833\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[1;32m-> 2835\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m to_parquet(\n\u001b[0;32m   2836\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2837\u001b[0m     path,\n\u001b[0;32m   2838\u001b[0m     engine,\n\u001b[0;32m   2839\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   2840\u001b[0m     index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   2841\u001b[0m     partition_cols\u001b[38;5;241m=\u001b[39mpartition_cols,\n\u001b[0;32m   2842\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   2843\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2844\u001b[0m )\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\Neuro\\lib\\site-packages\\pandas\\io\\parquet.py:420\u001b[0m, in \u001b[0;36mto_parquet\u001b[1;34m(df, path, engine, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[0;32m    416\u001b[0m impl \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[0;32m    418\u001b[0m path_or_buf: FilePath \u001b[38;5;241m|\u001b[39m WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[1;32m--> 420\u001b[0m impl\u001b[38;5;241m.\u001b[39mwrite(\n\u001b[0;32m    421\u001b[0m     df,\n\u001b[0;32m    422\u001b[0m     path_or_buf,\n\u001b[0;32m    423\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m    424\u001b[0m     index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m    425\u001b[0m     partition_cols\u001b[38;5;241m=\u001b[39mpartition_cols,\n\u001b[0;32m    426\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    427\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    428\u001b[0m )\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, io\u001b[38;5;241m.\u001b[39mBytesIO)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\Neuro\\lib\\site-packages\\pandas\\io\\parquet.py:195\u001b[0m, in \u001b[0;36mPyArrowImpl.write\u001b[1;34m(self, df, path, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mwrite_to_dataset(\n\u001b[0;32m    187\u001b[0m             table,\n\u001b[0;32m    188\u001b[0m             path_or_handle,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    191\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    192\u001b[0m         )\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;66;03m# write to single output file\u001b[39;00m\n\u001b[1;32m--> 195\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mwrite_table(\n\u001b[0;32m    196\u001b[0m             table, path_or_handle, compression\u001b[38;5;241m=\u001b[39mcompression, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    197\u001b[0m         )\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\Neuro\\lib\\site-packages\\pyarrow\\parquet.py:1817\u001b[0m, in \u001b[0;36mwrite_table\u001b[1;34m(table, where, row_group_size, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, coerce_timestamps, allow_truncated_timestamps, data_page_size, flavor, filesystem, compression_level, use_byte_stream_split, data_page_version, use_compliant_nested_type, **kwargs)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1800\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ParquetWriter(\n\u001b[0;32m   1801\u001b[0m             where, table\u001b[38;5;241m.\u001b[39mschema,\n\u001b[0;32m   1802\u001b[0m             filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1815\u001b[0m             use_compliant_nested_type\u001b[38;5;241m=\u001b[39muse_compliant_nested_type,\n\u001b[0;32m   1816\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m writer:\n\u001b[1;32m-> 1817\u001b[0m         \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_group_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow_group_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1818\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1819\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path_like(where):\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\Neuro\\lib\\site-packages\\pyarrow\\parquet.py:677\u001b[0m, in \u001b[0;36mParquetWriter.write_table\u001b[1;34m(self, table, row_group_size)\u001b[0m\n\u001b[0;32m    672\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTable schema does not match schema used to create file: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    673\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtable:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m vs. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mfile:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    674\u001b[0m            \u001b[38;5;241m.\u001b[39mformat(table\u001b[38;5;241m.\u001b[39mschema, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema))\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m--> 677\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_group_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow_group_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\Neuro\\lib\\site-packages\\pyarrow\\_parquet.pyx:1420\u001b[0m, in \u001b[0;36mpyarrow._parquet.ParquetWriter.write_table\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\Neuro\\lib\\site-packages\\pyarrow\\error.pxi:118\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowNotImplementedError\u001b[0m: Support for codec 'snappy' not built"
     ]
    }
   ],
   "source": [
    "df = to_parquet(dfb,dfa,'lpc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430f9488",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
